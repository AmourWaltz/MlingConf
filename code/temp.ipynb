{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename dict.keys() \n",
    "\n",
    "\"NLI result\" -> \"NLI score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "inp_file = \"./../exp/trivia/evaluate/val_2k_en_pred_eval.json\"\n",
    "out_file = \"./../exp/trivia/evaluate/val_2k_en_pred_eval_temp.json\"\n",
    "\n",
    "data_pool = utils.read_json(inp_file)\n",
    "\n",
    "for data in data_pool:\n",
    "    data[\"NLI result\"] = data.pop(\"NLI score\")\n",
    "\n",
    "utils.write_json(out_file, data_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat jsonlines to json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import utils\n",
    "\n",
    "inp_file = \"./../exp/trivia/evaluate/val_2k_zh_pred_eval.json\"\n",
    "out_file = \"./../exp/trivia/evaluate/val_2k_zh_pred_eval_temp.json\"\n",
    "\n",
    "with open(inp_file, \"r\") as fr:\n",
    "    data_pool = [json.loads(line) for line in fr.readlines()]\n",
    "\n",
    "utils.write_json(out_file, data_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding answers in eval files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "lang = \"ja\"\n",
    "eval_file = \"./../exp/trivia/evaluate/val_2k_{}_pred_eval_en.json\".format(lang)\n",
    "data_file = \"./../exp/trivia/generate/val_2k_{}_gpt-3.5-turbo_T0.8.json\".format(lang)\n",
    "\n",
    "data_pool, eval_pool = utils.read_json(data_file), utils.read_json(eval_file)\n",
    "\n",
    "for idx, samp in enumerate(eval_pool):\n",
    "    assert samp[\"question_id\"] == data_pool[idx][\"question_id\"]\n",
    "    samp[\"generated answer\"] = data_pool[idx][\"prediction\"]\n",
    "    samp[\"gold answer\"] = data_pool[idx][\"answer\"]\n",
    "\n",
    "utils.write_json(eval_file, eval_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT return value tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "Country, region, or territory not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_z/088jf1x159v9xhz0shjj5t0r0000gn/T/ipykernel_53993/4280375339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chatgpt_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WorkSpace/NLP/Research/Projects/MlingConf/MlingConf/code/utils/__init__.py\u001b[0m in \u001b[0;36mget_chatgpt_info\u001b[0;34m(model_name, input_context, temperature, logprobs, persona_info)\u001b[0m\n\u001b[1;32m     32\u001b[0m         ],\n\u001b[1;32m     33\u001b[0m         \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mlogprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m                 ),\n\u001b[1;32m    706\u001b[0m                 \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             raise self.handle_error_response(\n\u001b[0;32m--> 766\u001b[0;31m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m             )\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: Country, region, or territory not supported"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 1.0\n",
    "input_context = \"# Question #: What's the captial city of France?\\n# Proposed Answer #: Paris\\n\\n\" \\\n",
    "                \"Is the proposed answer:\\nTrue\\nFalse\\nThe possible answer is: \"\n",
    "\n",
    "\n",
    "response = utils.get_chatgpt_info(model_name, input_context, temperature)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common ../data/common/mling_common.json\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x80 in position 359: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_z/088jf1x159v9xhz0shjj5t0r0000gn/T/ipykernel_71359/3933056300.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mdata_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdata_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x80 in position 359: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from para import lang_map\n",
    "\n",
    "data_dir = \"../data/{}/mling_{}.json\"\n",
    "output_path = \"../data/human/en_zh_{}_samples.json\"\n",
    "datasets = [\"common\", \"trivia\", \"sciq\", \"gsm8k\"]\n",
    "language_list = [\"Japanese\", \"French\", \"Thai\"]\n",
    "\n",
    "\n",
    "\n",
    "for language in language_list:\n",
    "    language = lang_map[language]\n",
    "    sample_list = []\n",
    "    for dataset in datasets:\n",
    "        data_path = data_dir.format(dataset, dataset)\n",
    "        print(dataset, data_path)\n",
    "        with open(data_path, \"r\") as fr:\n",
    "            data_pool = json.load(fr)\n",
    "        data_pool = random.choices(data_pool, 20)\n",
    "        for data in data_pool:\n",
    "            sample = {\n",
    "                \"question_id\": data[\"question_id\"],\n",
    "                \"question\": {\n",
    "                    \"en\": data[\"en\"][\"question\"],\n",
    "                    \"zh\": data[\"zh\"][\"question\"],\n",
    "                    language: data[language][\"question\"]\n",
    "                },\n",
    "                \"answer\": {\n",
    "                    \"en\": data[\"en\"][\"answer\"],\n",
    "                    \"zh\": data[\"zh\"][\"answer\"],\n",
    "                    language: data[language][\"answer\"]\n",
    "                },\n",
    "                \"data_source\": dataset\n",
    "            }\n",
    "            sample_list.append(sample)\n",
    "    output_path = output_path.format(language)\n",
    "    with open(output_path, \"w\") as fw:\n",
    "        json.dump(fp=fw, obj=sample_list, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import backoff\n",
    "import openai\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from ipdb import set_trace\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as fr:\n",
    "        data_pool = json.load(fr)\n",
    "\n",
    "    return data_pool\n",
    "\n",
    "\n",
    "def get_chatgpt_info(model_name: str, input_context: str, temperature: float) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an excellent question responder.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_context,\n",
    "            },\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "prompt_form = \"icl\"\n",
    "temperature = 0.8\n",
    "test_set = \"test_1k\"\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "template_path = \"../prompt/selfaware/template.json\"\n",
    "prompt_path = \"../prompt/selfaware/\" + prompt_form + \".txt\"\n",
    "data_path = \"../data/selfaware/\" + test_set + \".json\"\n",
    "exp_path = \"../exp/selfaware\"\n",
    "output_path = os.path.join(exp_path, \"{}_{}_{}_T{}.json\".format(model_name, prompt_form, test_set, temperature))\n",
    "\n",
    "open(output_path, \"w\").close()\n",
    "\n",
    "template_input = json.load(open(template_path))\n",
    "prompt = open(prompt_path, \"r\").read()\n",
    "    \n",
    "data_list = read_json(data_path)\n",
    "length = len(data_list)\n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "print(\"Model Name:\", model_name, \"Temperature:\", temperature, \"Prompt Form:\", prompt_form)\n",
    "\n",
    "for idx in range(length):\n",
    "    question = data_list[idx]\n",
    "        \n",
    "    input_context = template_input[\"prompt_input\"].format(prompt, question)\n",
    "#     print(input_context)\n",
    "    generated_text = get_chatgpt_info(model_name, input_context, temperature)\n",
    "\n",
    "    data_list[idx][\"generation\"] = generated_text\n",
    "    with jsonlines.open(output_path, mode=\"a\") as writer:\n",
    "        writer.write(data_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "input_path = \"./../data/gsm8k/mling_gsm8k.json\"\n",
    "output_path = \"./../data/gsm8k/mling_gsm8k.json\"\n",
    "\n",
    "lang_list = [\"zh\", \"ja\", \"th\", \"fr\"]\n",
    "\n",
    "data_pool = utils.read_json(input_path)\n",
    "for data in data_pool:\n",
    "    answer = data[\"answer\"][\"en\"].split(\"####\")[-1]\n",
    "    # print(answer)\n",
    "    for lang in lang_list:\n",
    "        data[\"answer\"][lang] += \"\\n####\" + answer\n",
    "        # print(data[\"answer\"][lang])\n",
    "\n",
    "utils.write_json(output_path, data_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import jsonl2json\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "dataset = \"gsm8k\"\n",
    "data_path = f\"./../exp/{dataset}/inference/{model}_{dataset}_infer/generate.json\"\n",
    "\n",
    "jsonl2json(data_path, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import jsonl2json\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "dataset = \"triviaqa\"\n",
    "data_path = f\"./../exp/{dataset}/confidence/{model}_{dataset}_confidence/confidence.json\"\n",
    "\n",
    "jsonl2json(data_path, data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
